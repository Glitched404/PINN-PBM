{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN-PBM Colab Training Notebook\n",
    "Run the cells in order to train the breakage PINN on Google Colab with GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "1. **Runtime ▸ Change runtime type ▸ GPU** (required).\n",
    "2. Run the GPU check cell.\n",
    "3. Run the setup cell to clone the repo (or pull updates).\n",
    "4. Install dependencies.\n",
    "5. Execute the training cell (takes ~6 minutes on T4).\n",
    "6. Run the evaluation cell to visualize predictions vs. analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# Check that Colab sees a GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone (or update) the PINN-PBM repository\n",
    "from pathlib import Path\n",
    "\n",
    "repo_url = \"https://github.com/Glitched404/PINN-PBM.git\"\n",
    "repo_name = \"PINN-PBM\"\n",
    "repo_path = Path(repo_name)\n",
    "\n",
    "if not repo_path.exists():\n",
    "    !git clone $repo_url\n",
    "else:\n",
    "    print(\"Repository already exists. Pulling latest changes...\")\n",
    "    %cd $repo_name\n",
    "    !git pull\n",
    "    %cd ..\n",
    "\n",
    "%cd $repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install project dependencies (Python 3.11 compatible stack)\n",
    "%pip install -q \\\n",
    "    tensorflow==2.17.0 \\\n",
    "    tensorflow-probability==0.25.0 \\\n",
    "    numpy==1.26.4 \\\n",
    "    scipy==1.12.0 \\\n",
    "    matplotlib==3.8.2 \\\n",
    "    typing-extensions>=4.7.0,<5.0.0 \\\n",
    "    tqdm>=4.66.0,<5.0.0 \\\n",
    "    PyYAML>=6.0.0,<7.0.0 \\\n",
    "    pytest>=7,<9 \\\n",
    "    pytest-cov>=4,<5\n",
    "\n",
    "# Verify key packages after installation\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"tensorflow:\", tf.__version__)\n",
    "print(\"tensorflow_probability:\", tfp.__version__)\n",
    "\n",
    "# Ensure no dependency conflicts remain\n",
    "!pip check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_case1"
   },
   "outputs": [],
   "source": [
    "# Train BreakagePINN on Case 1 for a short run\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from pinn_pbm.breakage.models import BreakagePINN\n",
    "from pinn_pbm.breakage.solutions import get_analytical_solution\n",
    "from pinn_pbm.core.utils import set_random_seed, configure_gpu_memory_growth\n",
    "\n",
    "set_random_seed(42)\n",
    "configure_gpu_memory_growth()\n",
    "\n",
    "# Domain configuration\n",
    "v_min, v_max = 1e-3, 10.0\n",
    "t_min, t_max = 0.0, 10.0\n",
    "case_type = \"case1\"\n",
    "\n",
    "# Generate supervised data from the analytical solution\n",
    "v_points = np.logspace(np.log10(v_min), np.log10(v_max), 41, dtype=np.float32)\n",
    "t_slices = np.array([0.0, 2.0, 5.0, 10.0], dtype=np.float32)\n",
    "V_grid, T_grid = np.meshgrid(v_points, t_slices)\n",
    "F_grid = get_analytical_solution(V_grid, T_grid, case_type)\n",
    "\n",
    "v_train = V_grid.flatten().astype(np.float32)\n",
    "t_train = T_grid.flatten().astype(np.float32)\n",
    "f_train = F_grid.flatten().astype(np.float32)\n",
    "num_data = v_train.shape[0]\n",
    "\n",
    "# Collocation candidates for physics loss\n",
    "rng = np.random.default_rng(42)\n",
    "n_colloc_candidates = 20000\n",
    "v_colloc_candidates = np.exp(\n",
    "    rng.uniform(np.log(v_min), np.log(v_max), n_colloc_candidates)\n",
    ").astype(np.float32)\n",
    "t_colloc_candidates = rng.uniform(t_min + 1e-3, t_max, n_colloc_candidates).astype(np.float32)\n",
    "\n",
    "# Instantiate the PINN\n",
    "pinn = BreakagePINN(\n",
    "    v_min=v_min,\n",
    "    v_max=v_max,\n",
    "    t_min=t_min,\n",
    "    t_max=t_max,\n",
    "    case_type=case_type,\n",
    "    n_hidden_layers=8,\n",
    "    n_neurons=128,\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "\n",
    "num_epochs = 500\n",
    "batch_size_data = 512\n",
    "batch_size_phys = 1024\n",
    "target_w_phys = 0.01\n",
    "ramp_epochs = 200\n",
    "\n",
    "history = []\n",
    "\n",
    "for epoch in trange(num_epochs, desc=\"Training\"):\n",
    "    data_idx = rng.integers(0, num_data, size=batch_size_data)\n",
    "    v_data = tf.constant(v_train[data_idx])\n",
    "    t_data = tf.constant(t_train[data_idx])\n",
    "    f_data = tf.constant(f_train[data_idx][:, None])\n",
    "\n",
    "    colloc_idx = rng.integers(0, n_colloc_candidates, size=batch_size_phys)\n",
    "    v_phys = tf.constant(v_colloc_candidates[colloc_idx])\n",
    "    t_phys = tf.constant(t_colloc_candidates[colloc_idx])\n",
    "\n",
    "    w_phys_value = target_w_phys * min((epoch + 1) / ramp_epochs, 1.0)\n",
    "\n",
    "    total_loss, data_loss, phys_loss = pinn.train_step(\n",
    "        v_data=v_data,\n",
    "        t_data=t_data,\n",
    "        f_data=f_data,\n",
    "        v_physics=v_phys,\n",
    "        t_physics=t_phys,\n",
    "        w_data=tf.constant(1.0, dtype=tf.float32),\n",
    "        w_physics=tf.constant(w_phys_value, dtype=tf.float32),\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "\n",
    "    history.append((\n",
    "        float(total_loss.numpy()),\n",
    "        float(data_loss.numpy()),\n",
    "        float(phys_loss.numpy()),\n",
    "        w_phys_value,\n",
    "    ))\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        total, data, phys, w_val = history[-1]\n",
    "        print(f\"Epoch {epoch + 1:4d}: total={total:.3e}, data={data:.3e}, phys={phys:.3e}, w_phys={w_val:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_case1"
   },
   "outputs": [],
   "source": [
    "# Evaluate and visualize predictions vs analytical solution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "v_plot = np.logspace(np.log10(v_min), np.log10(v_max), 200, dtype=np.float32)\n",
    "t_eval = np.array([0.0, 2.0, 5.0, 10.0], dtype=np.float32)\n",
    "\n",
    "f_pred = pinn.predict(v_plot, t_eval)\n",
    "f_exact = np.array([get_analytical_solution(v_plot, float(t), case_type) for t in t_eval])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(t_eval)))\n",
    "\n",
    "for i, (ax, t_val, color) in enumerate(zip(axes, t_eval, colors)):\n",
    "    ax.semilogx(v_plot, f_exact[i], color=color, lw=2.5, label=\"Analytical\")\n",
    "    ax.semilogx(v_plot, f_pred[i], 'r--', lw=2.0, label=\"PINN\")\n",
    "    ax.set_title(f\"t = {t_val:.1f}\")\n",
    "    ax.set_xlabel('Volume v')\n",
    "    ax.set_ylabel('f(v, t)')\n",
    "    ax.grid(True, which='both', ls='--', alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Case 1: PINN vs Analytical Solution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report mean relative error for each time slice\n",
    "for i, t_val in enumerate(t_eval):\n",
    "    rel_err = np.abs((f_pred[i] - f_exact[i]) / (f_exact[i] + 1e-30))\n",
    "    print(f\"t={t_val:.1f}: mean relative error = {rel_err.mean():.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Adjust `num_epochs`, learning rates, and batch sizes for higher accuracy.\n",
    "- Swap `case_type` to `'case2'`, `'case3'`, or `'case4'` and update training data accordingly.\n",
    "- Save results to Google Drive or download with `files.download(...)` if needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
